{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rgb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMttv2BB3qFAa385c1XaUkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/rgb/blob/master/rgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P_eXSthuKDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy\n",
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yLqU7V0uWSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from flair.tokenization import SegtokTokenizer\n",
        "import time"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkV5IaR5uYmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# edit this to be a list of string you wish to correct.\n",
        "# to guarantee all formats of string are supported, it is suggested\n",
        "# you surround the strings with triple quotes (e.g. \"\"\" some text \"\"\")\n",
        "# instead of the typical 'some text' or \"some text\". this will allow\n",
        "# multi-line strings and avoids clashes with internal ' and \" characters.\n",
        "# if the text ends with a \" character you will still need to modify it to\n",
        "# \\\" because it will read the first three \" characters, then the fourth\n",
        "# will confuse it.\n",
        "text_list = [\"\"\"this is\n",
        "some 'text'\"\"\",\n",
        "\"\"\"who is he speaking to?\"\"\",\n",
        "\"\"\"who's he speaking to?\"\"\",\n",
        "\"\"\"He shouted at The Who\"\"\",\n",
        "\"\"\"He shouted at who?\"\"\"]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O0M0kvouhy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "tagger = SequenceTagger.load('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEEcTv__ukBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return start of corrected phrase, end of phrase, and whether starts with whom.\n",
        "def i_span(doc, token):\n",
        "\n",
        "    i_token = token.i\n",
        "    i_head = token.head.i\n",
        "\n",
        "    if i_token < i_head:\n",
        "        return i_token, i_head, True\n",
        "    else:\n",
        "        return i_head, i_token, False"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhukbFluqDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checks for various surrounding tokens which produce false flags\n",
        "# due to the typically terrible overall grammar and prevelance of\n",
        "# typos in forum posts written with these.\n",
        "def check_for_exceptions(doc, token):\n",
        "\n",
        "    i = token.i\n",
        "\n",
        "    if i > 0 and doc[i - 1].text.lower() in ['the', '\\\"']:\n",
        "        return True\n",
        "\n",
        "    if len(doc) > i + 1:\n",
        "        if doc[i + 1].text.lower in ['tf', '\\\"']:\n",
        "            return True\n",
        "        if doc[i + 1].text[0] == '\\'':\n",
        "            return True\n",
        "\n",
        "    if len(doc) > i + 2:\n",
        "        the_check = doc[i + 1].text.lower() in ['the', 'teh', 'th3', 't3h', 'da', 'd4', 'tha', 'th4', 't']\n",
        "        fuck_check = doc[i + 2].text.lower() in ['fuck', 'fck', 'fk', 'f', 'fuuck', 'fuuuck', 'fuuuuck', 'fuuuuuck']\n",
        "\n",
        "        if the_check and fuck_check:\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uiSkP7_uyCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use all caps or Spongebob-case if being used, otherwise append lowercase m.\n",
        "# surrounds with * on each side to emphasize the corrected word - on reddit\n",
        "# this italicizes the word.\n",
        "def whom_string(who_string):\n",
        "    if who_string == 'WHO':\n",
        "        return '*WHOM*'\n",
        "    elif who_string == 'wHo':\n",
        "        return '*wHoM*'\n",
        "    else:\n",
        "        return '*' + who_string + 'm*'"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk10CG3Tu1Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checks if 'who' is used where 'whom' should be used (i.e. as an object), and\n",
        "# for each such instance prints the text along with a correction to 'whom'\n",
        "# either starting at 'whom' and ending with the relevant verb/root, or vice versa.\n",
        "def correct_who_to_whom(text):\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    phrases = []\n",
        "\n",
        "    token_number = 0\n",
        "\n",
        "    for token in doc:\n",
        "\n",
        "        token_number += 1\n",
        "\n",
        "        # it is very difficult for named entity recognizer to recognize 'Who'\n",
        "        # in isolation - the motivating text was repeated exclamation of\n",
        "        # 'Who! Who!' in a The Grinch fan fiction.\n",
        "        if token.text.lower() in ['grinch', 'whoville']:\n",
        "            return\n",
        "\n",
        "        if token.text.lower() == 'who' :\n",
        "            if token.dep_ in ['dobj', 'iobj', 'pobj']:\n",
        "\n",
        "                # check for the hard-coded exceptions\n",
        "                if not check_for_exceptions(doc, token):\n",
        "                    print(doc[token.i + 1].text.lower())\n",
        "                    should_be_whom = True\n",
        "\n",
        "                    sentence = Sentence(text, use_tokenizer=SegtokTokenizer())\n",
        "                    tagger.predict(sentence)\n",
        "\n",
        "                    # make sure it is not part of a named entity\n",
        "                    for entity in sentence.get_spans('ner'):\n",
        "                        if token.idx >= entity.start_pos and token.idx <= entity.end_pos:\n",
        "                            should_be_whom = False\n",
        "\n",
        "                    if should_be_whom:\n",
        "\n",
        "                        phrase_start, phrase_end, whom_first = i_span(doc, token)\n",
        "\n",
        "                        if whom_first:\n",
        "                            # detokenizes the corrected excerpt (e.g. removes added space\n",
        "                            # between last word in sentence and punctutation, rejoins\n",
        "                            # don and 't to form don't, etc., only if such joins were\n",
        "                            # present in the original text)\n",
        "                            phrase = whom_string(token.text) + (''.join([tkn.text_with_ws for tkn in doc[phrase_start:phrase_end + 1]]))[3:]\n",
        "\n",
        "                            phrases.append(phrase)\n",
        "                        else:\n",
        "                            # detokenizes the corrected excerpt (e.g. removes added space\n",
        "                            # between last word in sentence and punctutation, rejoins\n",
        "                            # don and 't to form don't, etc., only if such joins were\n",
        "                            # present in the original text)\n",
        "                            phrase = ''.join([tkn.text_with_ws for tkn in doc[phrase_start:phrase_end]]) + whom_string(token.text)\n",
        "\n",
        "                            phrases.append(phrase)\n",
        "\n",
        "    # if any corrections were found, then print the original text and the corrections.\n",
        "    if phrases:\n",
        "        joined_phrases = '\\n\\n'.join(phrases)\n",
        "\n",
        "        print('<<<  TEXT  >>>')\n",
        "        print(text)\n",
        "        print('<<<  CORRECTIONS  >>>')\n",
        "        print(joined_phrases)\n",
        "        print()\n",
        "        print()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDSxG2_u6W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep track of submissions already replied to, to avoid buggy repeats\n",
        "#already_replied_to = []\n",
        "def main():\n",
        "\n",
        "    for text in text_list:\n",
        "        correct_who_to_whom(text)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AZ4bxWXu7zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}